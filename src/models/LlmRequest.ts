import {
  Content,
  Tool,
  SafetySetting,
  FunctionCall,
  FunctionResponse
} from './LlmContent.js';

/**
 * Configuration for the generation process of an LLM.
 */
export interface GenerationConfig {
  candidateCount?: number;
  stopSequences?: string[];
  maxOutputTokens?: number;
  temperature?: number;
  topP?: number;
  topK?: number;
  // Add other relevant generation parameters, e.g., presencePenalty, frequencyPenalty
}

/**
 * Represents a request to an LLM.
 */
export interface LlmRequest {
  /** The model name to use (e.g., 'gemini-1.5-pro-latest'). */
  model: string;

  /** The conversation history or prompt. */
  contents: Content[]; 

  /** Tools the model can use. */
  tools?: Tool[];

  /** Configuration for content generation. */
  generationConfig?: GenerationConfig;

  /** Safety settings for the request. */
  safetySettings?: SafetySetting[];

  /** 
   * Optional context about the system or instructions for the model.
   * Some models use a specific 'system' message type within contents for this.
   */
  systemInstruction?: Content; // Or string, depending on how it integrates with `contents`

  /** 
   * Tool configuration, like how to handle function calls (ANY, AUTO, NONE).
   * This might be part of a more specific ToolConfig object if complex.
   */
  toolConfig?: {
    mode?: 'ANY' | 'AUTO' | 'NONE'; // Simplified, specific to some models like Gemini
    functionCallingConfig?: {
      mode: 'ANY' | 'AUTO' | 'NONE';
      allowedFunctionNames?: string[];
    }
  };

  // Additional fields from Python LlmRequest that might be needed:
  // - request_id: string (could be auto-generated or passed in)
  // - app_name: string (for logging/tracking, might come from InvocationContext)
  // - timeout_sec: number
  // - extra_params: Record<string, any> (for model-specific parameters)
  requestId?: string;
  appName?: string;
  timeoutSec?: number;
  extraParams?: Record<string, any>;

  /**
   * Internal field to carry forward original function calls that led to tool use,
   * so the response processor can link FunctionResponse parts back.
   * Keyed by a unique ID (e.g., generated by the request processor).
   */
  _originalFunctionCalls?: Record<string, FunctionCall>;

  /**
   * Internal field to carry forward tool code execution results that need to be
   * sent back to the LLM as FunctionResponse parts.
   * Keyed by the same unique ID as _originalFunctionCalls.
   */
  _toolCodeExecutionResults?: Record<string, any>; // 'any' should be a structured result type

  /**
   * Internal field to carry data for auth requested by tools.
   * This was `_requested_auth_configs` in the Python version, part of `EventActions`.
   * Keeping it here if the LLM request itself needs to be modified based on auth flow.
   */
  _requestedAuthConfigs?: Record<string, any>; // Keyed by original function call ID
} 